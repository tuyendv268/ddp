{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tuyendv/Desktop/reranker\n"
     ]
    }
   ],
   "source": [
    "%cd /home/tuyendv/Desktop/reranker\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from argparse import ArgumentParser\n",
    "import openai\n",
    "import gradio as gr\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from model import Cross_Model\n",
    "from importlib.machinery import SourceFileLoader\n",
    "from transformers import RobertaModel\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from bm25 import BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_and_tokenizer(config):\n",
    "    AUTH_TOKEN = \"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\"\n",
    "    if config.general.plm == \"envibert\":\n",
    "        tokenizer = SourceFileLoader(\n",
    "            \"envibert.tokenizer\", \n",
    "            os.path.join(config.path.pretrained_dir,'envibert_tokenizer.py')) \\\n",
    "                .load_module().RobertaTokenizer(config.path.pretrained_dir)\n",
    "        plm = RobertaModel.from_pretrained(config.path.pretrained_dir)\n",
    "    elif config.general.plm == \"xlmr\":\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            'nguyenvulebinh/vi-mrc-base', cache_dir=config.path.pretrained_dir, use_auth_token=AUTH_TOKEN)\n",
    "        plm = AutoModel.from_pretrained(\n",
    "            \"nguyenvulebinh/vi-mrc-base\", cache_dir=config.path.pretrained_dir, use_auth_token=AUTH_TOKEN)\n",
    "    \n",
    "    model = Cross_Model(\n",
    "        max_length=config.general.max_length, \n",
    "        batch_size=config.general.batch_size,\n",
    "        device=config.general.device,\n",
    "        tokenizer=tokenizer, model=plm)\n",
    "    \n",
    "    if os.path.exists(config.path.warm_up):\n",
    "        model.load_state_dict(torch.load(config.path.warm_up, map_location=\"cpu\"))\n",
    "        print(f\"load model state dict from {config.path.warm_up}\")\n",
    "        \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pretrained were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing 4 layer\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load(\"config.yaml\")\n",
    "model, tokenizer = init_model_and_tokenizer(config)\n",
    "bm25_model = BM25()\n",
    "bm25_model.load(\"bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Haaland nói gì tại London\"\n",
    "bm25_result = bm25_model.search(query=query, topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [sample[1] for sample in bm25_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0   đây là khoảnh khắc đặc biệt haaland nói tại l...   \n",
      "1  tiền đạo 22 tuổi được kỳ vọng tiếp tục bùng nổ...   \n",
      "2   trong mùa giải đầu tiên tại sân etihad haalan...   \n",
      "3  cũng không thể phủ nhận năng lực của guardiola...   \n",
      "4  saka cùng 11 mầm non khác được đề nghị các hìn...   \n",
      "\n",
      "                       query  \n",
      "0  haaland nói gì tại london  \n",
      "1  haaland nói gì tại london  \n",
      "2  haaland nói gì tại london  \n",
      "3  haaland nói gì tại london  \n",
      "4  haaland nói gì tại london  \n",
      "model score:  tensor([0.4969, 0.5224, 0.4956, 0.4839, 0.5002])\n",
      "rank:  tensor([1, 4, 0, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.4969, 0.5224, 0.4956, 0.4839, 0.5002]),\n",
       " tensor([0.4969, 0.5224, 0.4956, 0.4839, 0.5002]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ranking(\n",
    "    query=query,\n",
    "    texts=docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
